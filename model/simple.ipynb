{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "from typing import Dict, Text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class MovieLensModel(tfrs.Model):\n",
    "  # We derive from a custom base class to help reduce boilerplate. Under the hood,\n",
    "  # these are still plain Keras Models.\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      user_model: tf.keras.Model,\n",
    "      movie_model: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "    super().__init__()\n",
    "\n",
    "    # Set up user and movie representations.\n",
    "    self.user_model = user_model\n",
    "    self.movie_model = movie_model\n",
    "\n",
    "    # Set up a retrieval task.\n",
    "    self.task = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # Define how the loss is computed.\n",
    "\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "    return self.task(user_embeddings, movie_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset movie_lens/1m-ratings/0.1.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Andy\\tensorflow_datasets\\movie_lens\\1m-ratings\\0.1.0...\u001B[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', layout=Layout(width='…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d731b68d50364f808e737e52c3dc842f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', layout=Layout(width='20px'…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4506c4ae944f43c58db9aa2e533cef37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', layout=Layout…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6b65a3085f44605af577776f5cac319"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ff9ba534d4f4574aafe202b668e2d0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-299cff0b5c04>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Ratings data.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mratings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtfds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'movie_lens/1m-ratings'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msplit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"train\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;31m# Features of all the available movies.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mmovies\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtfds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'movie_lens/1m-movies'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msplit\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"train\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\u001B[0m in \u001B[0;36mdisallow_positional_args_dec\u001B[1;34m(fn, instance, args, kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     \u001B[0m_check_no_positional\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mismethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mallowed\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mallowed\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m     \u001B[0m_check_required\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m   \u001B[1;32mreturn\u001B[0m \u001B[0mdisallow_positional_args_dec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwrapped\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=no-value-for-parameter\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001B[0m\n\u001B[0;32m    369\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mdownload\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    370\u001B[0m     \u001B[0mdownload_and_prepare_kwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdownload_and_prepare_kwargs\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 371\u001B[1;33m     \u001B[0mdbuilder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdownload_and_prepare\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mdownload_and_prepare_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    372\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    373\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mas_dataset_kwargs\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\u001B[0m in \u001B[0;36mdisallow_positional_args_dec\u001B[1;34m(fn, instance, args, kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     \u001B[0m_check_no_positional\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mismethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mallowed\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mallowed\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m     \u001B[0m_check_required\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m   \u001B[1;32mreturn\u001B[0m \u001B[0mdisallow_positional_args_dec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwrapped\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=no-value-for-parameter\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001B[0m in \u001B[0;36mdownload_and_prepare\u001B[1;34m(self, download_dir, download_config)\u001B[0m\n\u001B[0;32m    372\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_from_directory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_data_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    373\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 374\u001B[1;33m           self._download_and_prepare(\n\u001B[0m\u001B[0;32m    375\u001B[0m               \u001B[0mdl_manager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdl_manager\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    376\u001B[0m               download_config=download_config)\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001B[0m in \u001B[0;36m_download_and_prepare\u001B[1;34m(self, dl_manager, download_config)\u001B[0m\n\u001B[0;32m   1015\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_download_and_prepare\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdl_manager\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdownload_config\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1016\u001B[0m     \u001B[1;31m# Extract max_examples_per_split and forward it to _prepare_split\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1017\u001B[1;33m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001B[0m\u001B[0;32m   1018\u001B[0m         \u001B[0mdl_manager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdl_manager\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1019\u001B[0m         \u001B[0mmax_examples_per_split\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdownload_config\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_examples_per_split\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001B[0m in \u001B[0;36m_download_and_prepare\u001B[1;34m(self, dl_manager, **prepare_split_kwargs)\u001B[0m\n\u001B[0;32m    949\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    950\u001B[0m       \u001B[1;31m# Prepare split will record examples associated to the split\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 951\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_prepare_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msplit_generator\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mprepare_split_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    952\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    953\u001B[0m     \u001B[1;31m# Update the info object with the splits.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001B[0m in \u001B[0;36m_prepare_split\u001B[1;34m(self, split_generator, max_examples_per_split)\u001B[0m\n\u001B[0;32m   1031\u001B[0m     writer = tfrecords_writer.Writer(self._example_specs, fpath,\n\u001B[0;32m   1032\u001B[0m                                      hash_salt=split_generator.name)\n\u001B[1;32m-> 1033\u001B[1;33m     for key, record in utils.tqdm(generator, unit=\" examples\",\n\u001B[0m\u001B[0;32m   1034\u001B[0m                                   total=split_info.num_examples, leave=False):\n\u001B[0;32m   1035\u001B[0m       \u001B[0mexample\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencode_example\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tqdm\\notebook.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__iter__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    227\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 228\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtqdm_notebook\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__iter__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    229\u001B[0m                 \u001B[1;31m# return super(tqdm...) will not catch exception\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    230\u001B[0m                 \u001B[1;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tqdm\\std.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1131\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1133\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0miterable\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1134\u001B[0m                 \u001B[1;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1135\u001B[0m                 \u001B[1;31m# Update and possibly print the progressbar.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow_datasets\\structured\\movielens.py\u001B[0m in \u001B[0;36m_generate_examples\u001B[1;34m(self, dir_path)\u001B[0m\n\u001B[0;32m    443\u001B[0m   ) -> Iterator[Tuple[int, Dict[str, Any]]]:\n\u001B[0;32m    444\u001B[0m     \u001B[1;34m\"\"\"Yields examples by calling the corresponding parsing function.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 445\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mex\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuilder_config\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparsing_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdir_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    446\u001B[0m       \u001B[1;32myield\u001B[0m \u001B[0mex\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow_datasets\\structured\\movielens_parsing.py\u001B[0m in \u001B[0;36mparse_1m_ratings_data\u001B[1;34m(dir_path)\u001B[0m\n\u001B[0;32m    127\u001B[0m   \u001B[0mratings_file_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdir_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'ratings.dat'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    128\u001B[0m   \u001B[1;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgfile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mGFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mratings_file_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'rb'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mratings_file\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 129\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mrow_num\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mline\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mratings_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    130\u001B[0m       \u001B[0mline\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcodecs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mline\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'ISO-8859-1'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    131\u001B[0m       \u001B[0muser_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmovie_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrating\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimestamp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'::'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m__next__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mflush\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001B[0m in \u001B[0;36mnext\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    204\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 205\u001B[1;33m     \u001B[0mretval\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    206\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mretval\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    207\u001B[0m       \u001B[1;32mraise\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\seniorproject\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001B[0m in \u001B[0;36mreadline\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    168\u001B[0m     \u001B[1;34mr\"\"\"Reads the next line, keeping \\n. At EOF, returns ''.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    169\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_preread_check\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 170\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_prepare_value\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_read_buf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    171\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mreadlines\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Ratings data.\n",
    "ratings = tfds.load('movie_lens/1m-ratings', split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load('movie_lens/1m-movies', split=\"train\")\n",
    "\n",
    "# Select the basic features.\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"]\n",
    "})\n",
    "movies = movies.map(lambda x: x[\"movie_title\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define user and movie vocab\n",
    "user_ids_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define user and movie models.\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 64)\n",
    "])\n",
    "movie_model = tf.keras.Sequential([\n",
    "    movie_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(movie_titles_vocabulary.vocab_size(), 64)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define your objectives.\n",
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "    movies.batch(512).map(movie_model)\n",
    "  )\n",
    ")\n",
    "model = MovieLensModel(user_model, movie_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# Train for 3 epochs.\n",
    "model.fit(ratings.batch(4096), epochs=15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use brute-force search to set up retrieval using the trained representations.\n",
    "index = tfrs.layers.ann.BruteForce(model.user_model)\n",
    "index.index(movies.batch(100).map(model.movie_model), movies)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get some recommendations.\n",
    "_, titles = index(np.array([\"42\"]))\n",
    "print(f\"Top 3 recommendations for user 42: {titles[0, :3]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "seniorproject",
   "language": "python",
   "display_name": "Python (seniorproject)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}